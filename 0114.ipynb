{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv6hlmUT7Kn1yoKv9FKgsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t8101349/Colab-/blob/master/0114.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk_9lUR0b6WV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HTTP代碼\n",
        "\n",
        "```python\n",
        "2開頭: 成功\n",
        "\n",
        "3開頭: 轉址\n",
        "對面的網址已經搬家了, 雖然妳還是輸入舊網址, 但她會自動幫你替換成新網址\n",
        "\n",
        "4開頭: 錯誤\n",
        "\n",
        "404: Not Found\n",
        "!!! 最常見: 網址打錯\n",
        "403: Forbidden\n",
        "!!! 原因1: ip ban\n",
        "!!! 通常是數次的惡意行為(可能是短時間送太多...)\n",
        "!!! 解法: 等待/換ip\n",
        "!!! 原因2: 妳學的不像瀏覽器\n",
        "!!! 解法: 學的像一點(headers完善)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "aF6luf9Pb-q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Header\n",
        "\n",
        "```python\n",
        "妳在送出網址的時候, 其實不只送出網址\n",
        "妳送出的是 網址+額外信息(request headers)\n",
        "\n",
        "妳在得到回應的時候, 其實不是只有得到回應\n",
        "回應+額外信息(response headers)\n",
        "```"
      ],
      "metadata": {
        "id": "RHztQlPacBYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!!! 下次檢討\n",
        "\n",
        "把整頁的的文章的圖片都下載, 並且放在不同資料夾裡\n",
        "!!! 網址是縮寫的\n",
        "!!! 有些文章是被刪文的\n",
        "\n"
      ],
      "metadata": {
        "id": "Ik0VYdg_9f70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as req\n",
        "import bs4\n",
        "import json\n",
        "import os\n",
        "\n",
        "def download_img(url):\n",
        "  #url = 'https://www.ptt.cc/bbs/Beauty/M.1736755829.A.02A.html'\n",
        "  r = req.Request(url)\n",
        "  r.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\")\n",
        "\n",
        "  # 檔名\n",
        "  dir_name = url.split('/')[-2] + '/' + url.split('/')[-1]\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)\n",
        "  #fpath = dir_name + \"/\" + f_name\n",
        "\n",
        "\n",
        "  # 如果沒有class的話\n",
        "  # 1. 找她老爸\n",
        "  # 2. 自己寫篩選方式(!!!!!)\n",
        "  allow_subname = [\"jpg\", \"jpeg\", \"png\", \"gif\"]\n",
        "\n",
        "  response = req.urlopen(r)\n",
        "  data = response.read().decode('utf-8') #檔案\n",
        "  html = bs4.BeautifulSoup(data, 'html.parser')\n",
        "  links = html.find_all('a')\n",
        "  for link in links:\n",
        "      href = link['href']\n",
        "      #print(link['href'])\n",
        "      if href.split('.')[-1].lower() in allow_subname:\n",
        "          #print(href)\n",
        "          imgurl = href\n",
        "          r = req.Request(imgurl)\n",
        "          r.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\")\n",
        "          img = req.urlopen(r)\n",
        "          # open一個新檔案來儲存\n",
        "          f_name = dir_name + '/' + href.split('/')[-1]\n",
        "          # 存到資料夾\n",
        "          # f = open(f_name, 'wb')\n",
        "          with open(f_name, 'wb') as f:\n",
        "            f.write(img.read())\n",
        "          # f.close()\n",
        "\n",
        "          #print(img.read())\n",
        "          #print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "ivnnxAA2cAP8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_page(url):\n",
        "  r = req.Request(url)\n",
        "  r.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\")\n",
        "  response = req.urlopen(r)\n",
        "  data = response.read().decode('utf-8') #檔案\n",
        "  html = bs4.BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "\n",
        "  posts = html.find_all('div', class_ = 'r-ent')\n",
        "  for post in posts:\n",
        "    link = post.find('a')\n",
        "    #print(link['href'])\n",
        "    link = 'https://www.ptt.cc' + link['href']\n",
        "    download_img(link)\n",
        "\n",
        "download_page('https://www.ptt.cc/bbs/Beauty/index3960.html')"
      ],
      "metadata": {
        "id": "FRDWpt5XBjkt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_all_page(url):"
      ],
      "metadata": {
        "id": "E8FAzB3BCyly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open\n",
        "import os\n",
        "# 如果這個資料夾不存在\n",
        "# 為何是os.path->下次說\n",
        "if not os.path.exists(\"test/test2\"):\n",
        "    # 把它創造出來\n",
        "    os.makedirs(\"test/test2\")\n",
        "\n",
        "f = open(\"test/test2/a.txt\", \"w\", encoding=\"utf-8\")\n",
        "f.write(\"abcede\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "bPGFm48c5Rfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as req\n",
        "import bs4\n",
        "import json\n",
        "import os\n",
        "\n",
        "url = 'https://www.ptt.cc/bbs/Beauty/M.1736755829.A.02A.html'\n",
        "r = req.Request(url)\n",
        "r.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\")\n",
        "\n",
        "# 檔名\n",
        "f_name = url.split('/')[-1]+\".json\"\n",
        "dir_name = url.split('/')[-2]\n",
        "\n",
        "if not os.path.exists(dir_name):\n",
        "  os.makedirs(dir_name)\n",
        "fpath = dir_name + \"/\" + f_name\n",
        "\n",
        "response = req.urlopen(r)\n",
        "data = response.read().decode('utf-8') #檔案\n",
        "html = bs4.BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "#article = html.find_all('span', class_ = 'article-meta-value')\n",
        "article = html.find_all('span', {'class' , 'article-meta-value'})\n",
        "#print(article)\n",
        "author = article[0].text\n",
        "if '(' in author:\n",
        "  authorlist = author.split('(')\n",
        "  author = authorlist[0]\n",
        "  nickname = authorlist[1].replace(')', '')\n",
        "else:\n",
        "  nickname = ''\n",
        "\n",
        "board = article[1].text\n",
        "title = article[2].text\n",
        "if title.startswith('Re:'):\n",
        "  title = title.replace('Re:', '')\n",
        "if title.startswith('['):\n",
        "  titlelist = title.split(']')\n",
        "  category = titlelist[0].replace('[', '')\n",
        "  title = titlelist[1].strip()\n",
        "else:\n",
        "  category = ''\n",
        "\n",
        "date = article[3].text\n",
        "# 存檔的時候肯定是來個JSON\n",
        "data = {\n",
        "    \"id\":author,\n",
        "    \"nickname\":nickname,\n",
        "    \"board\":board,\n",
        "    \"category\":category,\n",
        "    \"title\":title,\n",
        "    \"date\":date,\n",
        "    \"push\":[]\n",
        "}\n",
        "\n",
        "print(\"作者:\", author)\n",
        "print(\"當時暱稱:\", nickname)\n",
        "print(\"看板:\", board)\n",
        "print(\"分類:\", category)\n",
        "print(\"標題:\", title)\n",
        "print(\"時間:\", date)\n",
        "\n",
        "contents = html.find_all('div', {'class' , 'push'})\n",
        "#table = [] = data['push']\n",
        "for content in contents:\n",
        "  push_data = {\n",
        "      'tag': content.find('span', {'class' , 'push-tag'}).text,\n",
        "      'user': content.find('span', {'class' , 'push-userid'}).text,\n",
        "      'content': content.find('span', {'class' , 'push-content'}).text,\n",
        "      'iptime': content.find('span', {'class' , 'push-ipdatetime'}).text\n",
        "  }\n",
        "  data['push'].append(push_data)\n",
        "print(data['push'])\n",
        "# Save the data to a JSON file\n",
        "with open(fpath, 'w', encoding='utf-8') as f:\n",
        "  json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t883kCzVsAeY",
        "outputId": "b20e2802-d645-4e0c-813e-fda3957f293f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "作者: Gentlemon \n",
            "當時暱稱: 肥宅紳士\n",
            "看板: Beauty\n",
            "分類: 正妹\n",
            "標題: Cosplay 1601 日本 約兒\n",
            "時間: Mon Jan 13 16:10:23 2025\n",
            "[{'tag': '推 ', 'user': 'jht', 'content': ': 大長腿', 'iptime': ' 223.138.43.159 01/13 17:00\\n'}, {'tag': '推 ', 'user': 'bio5chris', 'content': ': 好！！！', 'iptime': '223.138.217.143 01/13 20:30\\n'}, {'tag': '→ ', 'user': 'bingreen', 'content': ': 長', 'iptime': '  42.77.205.118 01/13 21:13\\n'}, {'tag': '推 ', 'user': 'sikadear', 'content': ': 讚', 'iptime': '  36.229.51.166 01/13 21:34\\n'}, {'tag': '→ ', 'user': 'sothyengiua', 'content': ': 可', 'iptime': ' 220.132.84.142 01/14 15:53\\n'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習: 拿其他的\n",
        "# 答案\n",
        "import os\n",
        "import json\n",
        "import urllib.request as req\n",
        "import bs4 as bs\n",
        "\n",
        "url = \"https://www.ptt.cc/bbs/Beauty/M.1736733511.A.738.html\"\n",
        "# 順便準備一下等一下要儲存的檔案名稱\n",
        "url_split = url.split(\"/\")\n",
        "fname = url_split[-1] + \".json\"\n",
        "dirname = url_split[-2]\n",
        "# 記得要先創資料夾, 不然紅字\n",
        "if not os.path.exists(dirname):\n",
        "    os.makedirs(dirname)\n",
        "fpath = dirname + \"/\" + fname\n",
        "\n",
        "r = req.Request(url)\n",
        "r.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\")\n",
        "\n",
        "response = req.urlopen(r)\n",
        "html = bs.BeautifulSoup(response)\n",
        "\n",
        "meta_list = html.find_all(\"span\", {\"class\":\"article-meta-value\"})\n",
        "\n",
        "ptt_id = meta_list[0].text\n",
        "if \"(\" in ptt_id:\n",
        "    ptt_id_split = ptt_id.split(\"(\")\n",
        "    ptt_id = ptt_id_split[0].strip()\n",
        "    nick_name = ptt_id_split[-1].replace(\")\", \"\")\n",
        "else:\n",
        "    nick_name = \"\"\n",
        "board = meta_list[1].text\n",
        "title = meta_list[2].text\n",
        "if title[0] == \"[\":\n",
        "    title_split = title.split(\"]\")\n",
        "    category = title_split[0].replace(\"[\", \"\")\n",
        "    title = title_split[-1].strip()\n",
        "else:\n",
        "    category = \"\"\n",
        "date = meta_list[3].text\n",
        "\n",
        "# 存檔的時候肯定是來個JSON\n",
        "data = {\n",
        "    \"id\":ptt_id,\n",
        "    \"nickname\":nick_name,\n",
        "    \"board\":board,\n",
        "    \"category\":category,\n",
        "    \"title\":title,\n",
        "    \"data\":date,\n",
        "    \"push\":[]\n",
        "}\n",
        "\n",
        "print(\"作者:\", ptt_id)\n",
        "print(\"當時暱稱:\", nick_name)\n",
        "print(\"看板:\", board)\n",
        "print(\"分類:\", category)\n",
        "print(\"標題:\", title)\n",
        "print(\"時間:\", date)\n",
        "\n",
        "\n",
        "push_list = html.find_all(\"div\", {\"class\":\"push\"})\n",
        "for push in push_list:\n",
        "    push_meta = push.find_all(\"span\")\n",
        "    push_type = push_meta[0].text.strip()\n",
        "    push_ptt_id = push_meta[1].text.strip()\n",
        "    push_content = push_meta[2].text.replace(\": \", \"\").strip()\n",
        "    push_ip_date = push_meta[3].text.strip()\n",
        "    print(push_type, push_ptt_id, push_content, push_ip_date)\n",
        "    push_data = {\n",
        "        \"type\":push_type,\n",
        "        \"id\":push_ptt_id,\n",
        "        \"content\":push_content,\n",
        "        \"ipdate\":push_ip_date\n",
        "    }\n",
        "    data[\"push\"].append(push_data)\n",
        "\n",
        "# 把json寫入到檔案裡\n",
        "f = open(fpath, \"w\", encoding=\"utf-8\")\n",
        "# 之前是f.write, 現在使用json.dump幫我做更好的write\n",
        "json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "oNI3nzRm23Yc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}